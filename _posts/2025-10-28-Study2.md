# Study2

현대의 IT 서비스 환경은 대규모 트래픽, 복잡한 마이크로서비스 구조, 다양한 외부 연계 시스템으로 인해 언제든지 장애가 발생할 수 있는 위험을 내포하고 있다. 특히 고객사 시스템의 경우, 비즈니스 임팩트가 크기 때문에 단순한 복구 수준을 넘어 **재발 방지와 구조적 개선(아키텍처 고도화)**이 필수적이다. 아래에서는 주요 장애 유형과 그에 따른 대응 및 개선 사례를 중심으로, 실질적인 대응 전략과 고도화 방안을 제시한다.

---

## 1️⃣ 서비스 과부하로 인한 응답 지연 및 타임아웃 발생

### 사례
고객사의 온라인 주문 시스템에서 특정 프로모션 기간 동안 접속자가 급증하면서 API 서버의 응답 속도가 급격히 느려지고, 일부 요청이 타임아웃으로 실패하는 현상이 발생하였다. 특히 특정 DB 조회 요청이 병목으로 작용하여 전체 응답 지연을 초래했다.

### 대응 방안
운영팀은 실시간 모니터링 시스템(Grafana, Loki, Prometheus)을 통해 CPU, Memory, DB 커넥션 수를 확인하였고, 특정 쿼리에서 Lock 경합이 발생함을 확인하였다. 즉시 임시적으로 DB read replica를 추가하여 부하를 분산시켰으며, API Gateway 레벨에서 캐싱을 활성화하여 동일 요청의 중복 처리를 줄였다.

### 고도화 방안
사후 분석을 통해 전체 트래픽 증가에 대비한 **오토스케일링 정책**이 미흡했음을 발견했다. 이에 따라 Kubernetes HPA(Horizontal Pod Autoscaler)를 도입하고, DB 쿼리 캐시 및 Redis 기반의 세션 분산 구조를 설계하였다. 또한, 트래픽 예측 모델을 기반으로 이벤트 전 사전 증설 프로세스를 자동화하였다.

---

## 2️⃣ 배치(Job) 처리 중 장애 및 스케줄링 충돌

### 사례
야간 데이터 마이그레이션 배치 작업이 특정 테이블을 장시간 Lock 상태로 유지하여, 실시간 트랜잭션 서비스에서 Deadlock 오류가 다수 발생하였다. 그 결과 일부 주문 데이터가 반영되지 않고 큐에 적체되는 현상이 발생하였다.

### 대응 방안
장애 발생 즉시 배치 작업을 중단하고, 큐에 남아있는 데이터를 우선 순위 기반으로 수동 처리하였다. 이후 배치 실행 주기 및 Lock 획득 구간을 재조정하여, 서비스 피크 시간대를 피하도록 조정하였다.

### 고도화 방안
Airflow 기반의 **스마트 스케줄링 시스템**을 도입하여 배치 간 의존성을 자동 분석하고 실행 순서를 최적화하였다. 또한, 배치 실행 중 Lock을 최소화하기 위해 파티션 테이블로 구조를 변경하고, CDC(Change Data Capture) 기반의 점진적 데이터 적재 방식을 적용하였다.

---

## 3️⃣ 외부 연계 시스템 장애로 인한 연쇄적 서비스 실패

### 사례
고객사의 결제 서비스가 외부 PG사와 연동되어 있는데, 해당 PG사의 응답 지연으로 인해 내부 결제 API가 타임아웃을 반복적으로 발생시키고 전체 트랜잭션 큐가 적체되었다. 이로 인해 내부 결제 요청 API도 동시에 느려지는 현상이 발생하였다.

### 대응 방안
장애 초기에는 외부 시스템 응답 대기시간을 줄이기 위해 타임아웃 설정을 하향 조정하고, 재시도 횟수를 제한하였다. 동시에 장애 감지 시 자동으로 fallback 로직(예: "결제 지연 중" 안내 메시지 제공)을 활성화하였다.

### 고도화 방안
**Circuit Breaker 패턴**을 적용하여 외부 연계 지연 시 자동으로 회로를 열고, 내부 시스템이 과도한 리소스를 소비하지 않도록 개선하였다. 또한, 메시지 큐(Kafka)를 이용해 비동기 결제 처리 구조를 도입하여 외부 장애에도 서비스 전체가 영향을 받지 않도록 설계하였다.

---

## 4️⃣ 애플리케이션 배포 중 장애 (CI/CD 관련 문제)

### 사례
운영 중이던 Spring Boot 애플리케이션의 신규 버전 배포 시, 일부 Pod에서 환경 변수 누락으로 인해 정상 구동되지 않았고, 이로 인해 서비스 일부 구간이 502 오류를 반환하였다.

### 대응 방안
배포 직후 장애를 감지한 후 즉시 이전 버전으로 롤백을 수행하였다. 이후 CI/CD 파이프라인 내 환경 변수 검증 단계를 추가하고, 배포 전 자동화된 헬스체크 및 Canary Release 방식을 적용하였다.

### 고도화 방안
GitOps 기반의 ArgoCD로 전환하여 **Declarative 배포 관리**를 수행하였으며, Helm Chart 템플릿에 모든 환경 변수를 명시하도록 일원화하였다. 또한, 배포 실패 시 자동 롤백 및 알림 기능을 추가하여 장애 대응 시간을 단축하였다.

---

## 5️⃣ 스토리지 장애 및 데이터 손상

### 사례
파일 업로드 서버의 NAS 스토리지 장애로 인해 일부 업로드 파일이 손상되거나 누락되는 문제가 발생하였다. 특히 업로드 중이던 사용자 파일의 Metadata가 DB에만 저장되고 실파일이 존재하지 않는 불일치 상태가 발생하였다.

### 대응 방안
스토리지 서버 복구 전 백업 서버에 저장된 최신 Snapshot을 복원하여 손실 데이터를 최소화하였다. 이후 손상된 파일 목록을 별도로 추출하여 사용자별로 복구 진행 여부를 안내하였다.

### 고도화 방안
파일 업로드 서비스를 S3 호환 오브젝트 스토리지(Minio) 기반으로 전환하여 **3중 복제 및 버전 관리 기능**을 활성화하였다. 또한, 파일 업로드 시 트랜잭션 일관성을 유지하기 위해 Event-driven 기반의 업로드 완료 검증 로직을 추가하였다.

---

## 6️⃣ 모니터링 및 알림 체계의 고도화

장애의 근본적인 대응 능력을 높이기 위해서는 **사전 감지와 자동 대응 체계**가 필수적이다. 이를 위해 Prometheus, Grafana, Loki, Alertmanager를 기반으로 하는 통합 모니터링 환경을 구축하였다.  
CPU, 메모리, 디스크 뿐 아니라 API 응답시간, 에러율, DB Connection Pool 사용률 등 **애플리케이션 레벨의 지표**를 추가 수집하였다.  
또한, 장애 발생 시 단순 Slack 알림을 넘어 자동으로 트리거되는 **Self-healing 스크립트**를 배포하여 일시적인 장애를 즉시 완화시킬 수 있도록 하였다.

---

## 7️⃣ 아키텍처 고도화 방향 요약

- **Microservice 기반 분리 및 독립 배포**: 장애 확산 최소화  
- **Event-driven Architecture (Kafka, RabbitMQ)**: 비동기 처리 및 성능 개선  
- **Auto Scaling & Self-healing**: 장애 자동 복구 및 트래픽 대응  
- **Zero-downtime Deployment**: Blue-Green / Canary 방식 적용  
- **Observability 강화**: 중앙 집중형 로그 및 트레이싱 (Loki + Jaeger)  
- **Resiliency 설계**: Circuit Breaker, Bulkhead, Retry, Timeout 등 적용  

---

## 📘 결론

시스템 장애는 완전히 예방할 수는 없지만, **예측 가능한 설계와 신속한 대응 체계**를 갖추면 장애의 영향 범위를 최소화할 수 있다.  
고객사의 시스템 역시 단순히 문제를 “해결”하는 수준을 넘어, **장애로부터 회복 가능한 구조(Resilient Architecture)**로 진화해야 한다.  
이를 위해 운영팀과 개발팀이 지속적으로 협력하며, 장애 발생 시마다 그 원인을 분석하고 구조적 개선으로 이어지는 **Post-mortem 문화**를 정착시키는 것이 궁극적인 고도화의 핵심이라 할 수 있다.